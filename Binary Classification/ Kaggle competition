{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":73291,"databundleVersionId":8930475,"sourceType":"competition"},{"sourceId":8856386,"sourceType":"datasetVersion","datasetId":5331400}],"dockerImageVersionId":30733,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ruba000/binary-classification-result?scriptVersionId=186805818\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, roc_auc_score, classification_report\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the datasets\ntrain = pd.read_csv('/kaggle/input/binary-classification/train.csv')\ntest = pd.read_csv('/kaggle/input/binary-classification/test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-07-04T09:28:48.191377Z","iopub.execute_input":"2024-07-04T09:28:48.191867Z","iopub.status.idle":"2024-07-04T09:28:52.873545Z","shell.execute_reply.started":"2024-07-04T09:28:48.191834Z","shell.execute_reply":"2024-07-04T09:28:52.872296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Explore the Data\nprint(train.head())\nprint(test.head())\n\n# Get basic information about the datasets\nprint(train.info())\nprint(test.info())\n\n# Describe the datasets to understand the distributions\nprint(train.describe())\nprint(test.describe())\n","metadata":{"execution":{"iopub.status.busy":"2024-07-04T09:34:31.127459Z","iopub.execute_input":"2024-07-04T09:34:31.127886Z","iopub.status.idle":"2024-07-04T09:34:31.549703Z","shell.execute_reply.started":"2024-07-04T09:34:31.127854Z","shell.execute_reply":"2024-07-04T09:34:31.548192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Exploratory Data Analysis (EDA)\n# Visualize Data\n# Plot distributions of features\ntrain.hist(bins=50, figsize=(20, 15))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-04T09:36:12.109757Z","iopub.execute_input":"2024-07-04T09:36:12.110172Z","iopub.status.idle":"2024-07-04T09:36:15.175737Z","shell.execute_reply.started":"2024-07-04T09:36:12.110139Z","shell.execute_reply":"2024-07-04T09:36:15.174579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Analyze Target Variable:\n# Check the distribution of the Response variable\nsns.countplot(x='Response', data=train)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-04T09:36:48.92699Z","iopub.execute_input":"2024-07-04T09:36:48.927392Z","iopub.status.idle":"2024-07-04T09:36:49.163669Z","shell.execute_reply.started":"2024-07-04T09:36:48.927359Z","shell.execute_reply":"2024-07-04T09:36:49.162488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Identify Patterns and Relationships\n# Use correlation matrices to identify relationships between features\nnumeric_columns = train.select_dtypes(include=['float64', 'int64'])\ncorr_matrix = numeric_columns.corr()\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-04T09:37:31.06161Z","iopub.execute_input":"2024-07-04T09:37:31.062416Z","iopub.status.idle":"2024-07-04T09:37:31.699635Z","shell.execute_reply.started":"2024-07-04T09:37:31.062373Z","shell.execute_reply":"2024-07-04T09:37:31.698498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data Cleaning and Feature Engineering\n# Handle Missing Values\n# Identify missing values\nprint(train.isnull().sum())\nprint(test.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2024-07-04T09:37:55.018814Z","iopub.execute_input":"2024-07-04T09:37:55.019813Z","iopub.status.idle":"2024-07-04T09:37:55.195178Z","shell.execute_reply.started":"2024-07-04T09:37:55.019775Z","shell.execute_reply":"2024-07-04T09:37:55.194028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ensure 'id' column in test data is present\nif 'id' not in test.columns:\n    raise KeyError(\"'id' column not found in test DataFrame.\")\n\n# Extract 'id' column into test_ids\ntest_ids = test['id']\n\n# Define the new IDs you want to use\nnew_ids = [11504798 + i for i in range(len(test))]\n\n# Ensure the length of new_ids matches the number of rows in the test dataset\nif len(new_ids) != len(test):\n    raise ValueError(\"The length of new_ids does not match the number of rows in the test dataset.\")\n\n# Replace the existing IDs in the test dataset with the new IDs\ntest_ids = new_ids\n\n# Drop 'id' column in the test file\ntest.drop('id', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T10:36:09.516699Z","iopub.execute_input":"2024-07-04T10:36:09.517262Z","iopub.status.idle":"2024-07-04T10:36:09.557947Z","shell.execute_reply.started":"2024-07-04T10:36:09.51722Z","shell.execute_reply":"2024-07-04T10:36:09.556494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Separate the target variable before one-hot encoding\nif 'Response' not in train.columns:\n    raise KeyError(\"Response column not found in train DataFrame.\")\n\ny = train['Response']\ntrain = train.drop('Response', axis=1)\n\n# Convert categorical columns into dummy variables\ncategorical_columns = ['Gender', 'Vehicle_Age', 'Vehicle_Damage']\n\n# Apply one-hot encoding directly with pandas\ntrain_encoded = pd.get_dummies(train, columns=categorical_columns, drop_first=True, dtype=int)\ntest_encoded = pd.get_dummies(test, columns=categorical_columns, drop_first=True, dtype=int)\n\n# Align the train and test dataframes to ensure they have the same columns\ntrain_encoded, test_encoded = train_encoded.align(test_encoded, join='inner', axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-04T09:44:16.068396Z","iopub.execute_input":"2024-07-04T09:44:16.06938Z","iopub.status.idle":"2024-07-04T09:44:16.35261Z","shell.execute_reply.started":"2024-07-04T09:44:16.069344Z","shell.execute_reply":"2024-07-04T09:44:16.351328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scale the data\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(train_encoded)\nX_test_scaled = scaler.transform(test_encoded)\n\n# Split data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X_train_scaled, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T09:44:42.253966Z","iopub.execute_input":"2024-07-04T09:44:42.254349Z","iopub.status.idle":"2024-07-04T09:44:42.427085Z","shell.execute_reply.started":"2024-07-04T09:44:42.25432Z","shell.execute_reply":"2024-07-04T09:44:42.425828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train and evaluate models with timing measurements\nmodels = {\n    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n    \"Decision Tree\": DecisionTreeClassifier(),\n    \"Random Forest\": RandomForestClassifier(),\n    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n}\n\nmodel_performance = {}\nmodel_times = {}  # Dictionary to store training times\n\nfor name, model in models.items():\n    start_time = time.time()  # Start timing\n    model.fit(X_train, y_train)\n    end_time = time.time()  # End timing\n    training_time = end_time - start_time  # Calculate training time\n    model_times[name] = training_time  # Store training time for the model\n    \n    y_pred = model.predict(X_val)\n    y_pred_proba = model.predict_proba(X_val)[:, 1]\n    accuracy = accuracy_score(y_val, y_pred)\n    roc_auc = roc_auc_score(y_val, y_pred_proba)\n    \n    print(f\"{name}\")\n    print(f\"Training Time: {training_time:.2f} seconds\")  # Print training time\n    print(f\"Accuracy: {accuracy:.2f}\")\n    print(f\"ROC AUC: {roc_auc:.2f}\")\n    print(classification_report(y_val, y_pred))\n    \n    model_performance[name] = roc_auc\n","metadata":{"execution":{"iopub.status.busy":"2024-07-04T10:48:45.564599Z","iopub.execute_input":"2024-07-04T10:48:45.565006Z","iopub.status.idle":"2024-07-04T10:49:59.761787Z","shell.execute_reply.started":"2024-07-04T10:48:45.564976Z","shell.execute_reply":"2024-07-04T10:49:59.760505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select the best model\nbest_model_name = max(model_performance, key=model_performance.get)\nbest_model = models[best_model_name]\n\nprint(f\"Best Model: {best_model_name} with ROC AUC: {model_performance[best_model_name]:.2f}\")\n\n# Make predictions on the test data with the best model\npredictions_test = best_model.predict_proba(X_test_scaled)[:, 1]# Select the best model based on ROC AUC\nbest_model_name = max(model_performance, key=model_performance.get)\nbest_model = models[best_model_name]\n\nprint(f\"Training Time for {best_model_name}: {model_times[best_model_name]:.2f} seconds\")\n\n# Make predictions on the test data with the best model\npredictions_test = best_model.predict_proba(X_test_scaled)[:, 1]","metadata":{"execution":{"iopub.status.busy":"2024-07-04T10:35:31.965545Z","iopub.execute_input":"2024-07-04T10:35:31.966483Z","iopub.status.idle":"2024-07-04T10:35:32.225242Z","shell.execute_reply.started":"2024-07-04T10:35:31.966442Z","shell.execute_reply":"2024-07-04T10:35:32.224294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Here is a comparison of the four models that I have trained, highlighting their strengths and why I choose XGBoost is the best model:**\n\n**Logistic Regression:**\n**Description**: A linear model for binary classification that predicts probabilities using a logistic function.\n**Strengths**:\n-Simple and interpretable.\n-Works well with linearly separable data.\n-Computationally efficient and fast.\n**When to Use**:\nWhen you need a simple, interpretable model.\nWhen the relationship between features and the target is roughly linear.\nFor baseline performance comparison.\n\n**Decision Tree:**\n**Description**: A tree-based model that splits data into subsets based on feature values to make predictions.\n**Strengths**:\n-Easy to interpret and visualize.\n-Captures non-linear relationships and interactions between features.\n-Requires little data preprocessing.\n**When to Use**:\nWhen interpretability is important.\nWhen you have complex, non-linear relationships in your data.\nWhen you need a model that can handle both numerical and categorical data without much preprocessing.\n\n**Random Forest:**\n**Description**: An ensemble method that combines multiple decision trees to improve accuracy and control overfitting.\n**Strengths**:\nReduces overfitting compared to a single decision tree.\nHandles non-linear relationships well.\nProvides feature importance scores.\n**When to Use:**\nWhen you need a robust model that generalizes well.\nWhen you have complex, non-linear data.\nWhen you need to handle high-dimensional data.\n\n**XGBoost**:\n**Description**: An optimized gradient boosting algorithm that builds an ensemble of decision trees in a sequential manner.\n**Strengths**:\nHigh predictive performance.\nHandles missing data well.\nSupports regularization to reduce overfitting.\nEfficient and scalable implementation.\n**When to Use:**\nWhen you need state-of-the-art performance.\nWhen you have a large dataset with complex relationships.\nWhen you need a model that can be fine-tuned extensively for better performance.\n\n**Findings**\nFor the binary classification project, where the goal is to predict which customers respond positively to an automobile insurance offer,so what is matter here is the performance, that's why I choose XGBoost as best model. \n\n**Conclusion**\nIf interpretability is crucial: Use Logistic Regression.\nIf handling complex relationships are more important: Use Random Forest or XGBoost.\n If you are looking for the best performance and don't mind spending time on tuning and possibly dealing with more computational complexity, XGBoost would be a better choice.","metadata":{}},{"cell_type":"code","source":"# Prepare the result DataFrame\nresult = pd.DataFrame({'id': test_ids, 'Response': predictions_test.flatten()}, columns=['id', 'Response'])\n\n# Save predictions to a CSV file\nresult.to_csv('test_predictions.csv', index=False)\n\nprint(result.head())","metadata":{"execution":{"iopub.status.busy":"2024-07-04T10:35:54.763912Z","iopub.execute_input":"2024-07-04T10:35:54.764411Z","iopub.status.idle":"2024-07-04T10:35:55.200717Z","shell.execute_reply.started":"2024-07-04T10:35:54.764369Z","shell.execute_reply":"2024-07-04T10:35:55.199625Z"},"trusted":true},"execution_count":null,"outputs":[]}]}